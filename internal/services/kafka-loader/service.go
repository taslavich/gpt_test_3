package kafka_loader

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"time"

	"github.com/redis/go-redis/v9"
	"github.com/segmentio/kafka-go"
	"gitlab.com/twinbid-exchange/RTB-exchange/internal/constants"
	"gitlab.com/twinbid-exchange/RTB-exchange/internal/types"
)

func ProcessBatch(ctx context.Context, redisClient *redis.Client, kafkaWriter *kafka.Writer, batchSize int64) error {
	totalKeys, err := redisClient.DBSize(ctx).Result()
	if err != nil {
		return fmt.Errorf("failed to get total keys count: %v", err)
	}

	if totalKeys < batchSize*2 {
		log.Printf("üì≠ Not enough stats records found in Redis")
		return nil
	}

	allKeys, err := redisClient.Keys(ctx, "stats:*").Result()
	if err != nil {
		return fmt.Errorf("failed to get stats keys: %v", err)
	}

	uuids := allKeys[:batchSize]

	pipe := redisClient.Pipeline()
	for _, uuid := range uuids {
		pipe.HGetAll(ctx, uuid)
	}

	cmds, err := pipe.Exec(ctx)
	if err != nil {
		return fmt.Errorf("failed to get data from Redis: %v", err)
	}

	var kafkaMessages []kafka.Message
	fieldsToDelete := make(map[string][]string)

	for i, cmd := range cmds {
		data, err := cmd.(*redis.MapStringStringCmd).Result()
		if err != nil {
			log.Printf("‚ö†Ô∏è Failed to get data for UUID %s: %v", uuids[i], err)
			continue
		}

		// –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≤–º–µ—Å—Ç–æ map
		record := types.StatisticsRecord{}
		key := uuids[i]

		if bidRequest, exists := data[constants.BID_REQUEST_COLUMN]; exists {
			record.BID_REQUEST = bidRequest
			fieldsToDelete[key] = append(fieldsToDelete[key], constants.BID_REQUEST_COLUMN)
		}

		if geoColumn, exists := data[constants.GEO_COLUMN]; exists {
			record.GEO_COLUMN = geoColumn
			fieldsToDelete[key] = append(fieldsToDelete[key], constants.GEO_COLUMN)
		}

		if bidResponses, exists := data[constants.BID_RESPONSES_COLUMN]; exists {
			record.BID_RESPONSES = bidResponses
			fieldsToDelete[key] = append(fieldsToDelete[key], constants.BID_RESPONSES_COLUMN)
		}

		if bidResponseWinner, exists := data[constants.BID_RESPONSE_WINNER_COLUMN]; exists {
			record.BID_RESPONSE_WINNER = bidResponseWinner
			fieldsToDelete[key] = append(fieldsToDelete[key], constants.BID_RESPONSE_WINNER_COLUMN)
		}

		if bidResponseWinnerByDspPrice, exists := data[constants.BID_RESPONSE_WINNER_BY_DSP_PRICE_COLUMN]; exists {
			record.BID_RESPONSE_WINNER_BY_DSP_PRICE = bidResponseWinnerByDspPrice
			fieldsToDelete[key] = append(fieldsToDelete[key], constants.BID_RESPONSE_WINNER_BY_DSP_PRICE_COLUMN)
		}

		if success, exists := data[constants.RESULT_COLUMN]; exists {
			record.SUCCESS = success
			fieldsToDelete[key] = append(fieldsToDelete[key], constants.RESULT_COLUMN)
		}

		// –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –≤ –∑–∞–ø–∏—Å–∏
		if hasData(record) {
			jsonData, err := json.Marshal(record)
			if err != nil {
				log.Printf("‚ùå Failed to marshal record for UUID %s: %v", uuids[i], err)
				continue
			}

			kafkaMessages = append(kafkaMessages, kafka.Message{
				Value: jsonData,
			})
		}
	}

	if len(kafkaMessages) > 0 {
		if err := kafkaWriter.WriteMessages(ctx, kafkaMessages...); err != nil {
			return fmt.Errorf("failed to write to Kafka: %v", err)
		}
	}

	if len(fieldsToDelete) > 0 {
		pipe := redisClient.Pipeline()

		for key, fields := range fieldsToDelete {
			pipe.HDel(ctx, key, fields...)
		}

		if _, err := pipe.Exec(ctx); err != nil {
			log.Printf("‚ö†Ô∏è Failed to delete some fields from Redis: %v", err)
		}
	}

	return nil
}

// –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞–ª–∏—á–∏—è –¥–∞–Ω–Ω—ã—Ö
func hasData(record types.StatisticsRecord) bool {
	return record.BID_REQUEST != "" ||
		record.GEO_COLUMN != "" ||
		record.BID_RESPONSES != "" ||
		record.BID_RESPONSE_WINNER != "" ||
		record.BID_RESPONSE_WINNER_BY_DSP_PRICE != "" ||
		record.SUCCESS != ""
}

func EnsureTopicExists(broker string, topic string) error {
	var conn *kafka.Conn
	var err error

	conn, err = kafka.Dial("tcp", broker)
	if err != nil {
		return fmt.Errorf("‚ö†Ô∏è Failed to connect to broker %s: %v", broker, err)
	}
	log.Printf("‚úÖ Connected to Kafka broker: %s", broker)
	defer conn.Close()

	if conn == nil {
		return fmt.Errorf("failed to connect to any Kafka broker: %v", broker)
	}

	// –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–∞ (–≤–µ–¥—É—â–∏–π –±—Ä–æ–∫–µ—Ä)
	controller, err := conn.Controller()
	if err != nil {
		return fmt.Errorf("failed to get controller: %v", err)
	}

	// –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä—É –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ–º—ã
	controllerConn, err := kafka.Dial("tcp", fmt.Sprintf("%s:%d", controller.Host, controller.Port))
	if err != nil {
		return fmt.Errorf("failed to connect to controller: %v", err)
	}
	defer controllerConn.Close()

	// –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ç–µ–º
	partitions, err := controllerConn.ReadPartitions()
	if err != nil {
		return fmt.Errorf("failed to read partitions: %v", err)
	}

	// –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ç–µ–º–∞
	for _, p := range partitions {
		if p.Topic == topic {
			log.Printf("‚úÖ Topic %s already exists", topic)
			return nil
		}
	}

	retentionMs := 5 * 60 * 60 * 1000 // 5 —á–∞—Å–æ–≤ –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö

	configs := []kafka.ConfigEntry{
		{
			ConfigName:  "retention.ms",
			ConfigValue: fmt.Sprintf("%d", retentionMs),
		},
		{
			ConfigName:  "cleanup.policy",
			ConfigValue: "delete",
		},
	}

	// –°–æ–∑–¥–∞–µ–º —Ç–µ–º—É –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
	topicConfigs := []kafka.TopicConfig{
		{
			Topic:             topic,
			NumPartitions:     1,
			ReplicationFactor: 1,
			ConfigEntries:     configs,
		},
	}

	err = controllerConn.CreateTopics(topicConfigs...)
	if err != nil {
		return fmt.Errorf("failed to create topic: %v", err)
	}

	log.Printf("‚úÖ Created topic: %s with %d partitions", topic, 3)

	// –î–∞–µ–º –≤—Ä–µ–º—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ–º—ã
	time.Sleep(2 * time.Second)
	return nil
}
